{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the first HIV related websites file (positive class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "pos_class_list1 = []\n",
    "with open(\"pos_class.jsonl\") as json_file:\n",
    "   for line in json_file:\n",
    "    pos_class_list1.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>text_dump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.hivsharespace.net</td>\n",
       "      <td>Skip to main content En Fr Pt HOME | ABOUT | C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.dapp-namibia.org</td>\n",
       "      <td>Register \\r\\n                                 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.pepfar.gov</td>\n",
       "      <td>Skip to Main Content PEPFAR Seal The United St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.arasa.info</td>\n",
       "      <td>Home AIDS 2016 About Staff Office Addresses Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://hivsa.com</td>\n",
       "      <td>facebook twitter linkedin youtube Home Who We ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         domain  \\\n",
       "0  http://www.hivsharespace.net   \n",
       "1   http://www.dapp-namibia.org   \n",
       "2        https://www.pepfar.gov   \n",
       "3         http://www.arasa.info   \n",
       "4              http://hivsa.com   \n",
       "\n",
       "                                           text_dump  \n",
       "0  Skip to main content En Fr Pt HOME | ABOUT | C...  \n",
       "1  Register \\r\\n                                 ...  \n",
       "2  Skip to Main Content PEPFAR Seal The United St...  \n",
       "3  Home AIDS 2016 About Staff Office Addresses Co...  \n",
       "4  facebook twitter linkedin youtube Home Who We ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe from the pos_class_list1: pos_sites1\n",
    "pos_sites1= pd.DataFrame(pos_class_list1)\n",
    "pos_sites1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>text_dump</th>\n",
       "      <th>HIV_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.hivsharespace.net</td>\n",
       "      <td>Skip to main content En Fr Pt HOME | ABOUT | C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.dapp-namibia.org</td>\n",
       "      <td>Register \\r\\n                                 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.pepfar.gov</td>\n",
       "      <td>Skip to Main Content PEPFAR Seal The United St...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.arasa.info</td>\n",
       "      <td>Home AIDS 2016 About Staff Office Addresses Co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://hivsa.com</td>\n",
       "      <td>facebook twitter linkedin youtube Home Who We ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         domain  \\\n",
       "0  http://www.hivsharespace.net   \n",
       "1   http://www.dapp-namibia.org   \n",
       "2        https://www.pepfar.gov   \n",
       "3         http://www.arasa.info   \n",
       "4              http://hivsa.com   \n",
       "\n",
       "                                           text_dump  HIV_related  \n",
       "0  Skip to main content En Fr Pt HOME | ABOUT | C...            1  \n",
       "1  Register \\r\\n                                 ...            1  \n",
       "2  Skip to Main Content PEPFAR Seal The United St...            1  \n",
       "3  Home AIDS 2016 About Staff Office Addresses Co...            1  \n",
       "4  facebook twitter linkedin youtube Home Who We ...            1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a label column of 1s indicating that the sites are HIV related sites\n",
    "pos_sites1[\"HIV_related\"]=1\n",
    "pos_sites1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the first NOT HIV related sites folder (Negative Class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>text_dump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://stackoverflow.com</td>\n",
       "      <td>Stack Overflow Questions Developer Jobs Tags U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://en.wikipedia.org</td>\n",
       "      <td>Cat From Wikipedia, the free encyclopedia \\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.farming-simulator.com</td>\n",
       "      <td>Updates Support Australia (en) Belgien (de) Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.canva.com</td>\n",
       "      <td>+1 335  Tweet 714  Share 8.5K  Pin 28.4K  Sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hello.com</td>\n",
       "      <td>about us inspiration graphic novel best of hel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              domain  \\\n",
       "0          https://stackoverflow.com   \n",
       "1           https://en.wikipedia.org   \n",
       "2  https://www.farming-simulator.com   \n",
       "3              https://www.canva.com   \n",
       "4                  https://hello.com   \n",
       "\n",
       "                                           text_dump  \n",
       "0  Stack Overflow Questions Developer Jobs Tags U...  \n",
       "1  Cat From Wikipedia, the free encyclopedia \\n\\t...  \n",
       "2  Updates Support Australia (en) Belgien (de) Be...  \n",
       "3   +1 335  Tweet 714  Share 8.5K  Pin 28.4K  Sha...  \n",
       "4  about us inspiration graphic novel best of hel...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_sites1=pd.read_json(\"neg_class.jsonl\", lines=True)\n",
    "neg_sites1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>text_dump</th>\n",
       "      <th>HIV_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://stackoverflow.com</td>\n",
       "      <td>Stack Overflow Questions Developer Jobs Tags U...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://en.wikipedia.org</td>\n",
       "      <td>Cat From Wikipedia, the free encyclopedia \\n\\t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.farming-simulator.com</td>\n",
       "      <td>Updates Support Australia (en) Belgien (de) Be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.canva.com</td>\n",
       "      <td>+1 335  Tweet 714  Share 8.5K  Pin 28.4K  Sha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hello.com</td>\n",
       "      <td>about us inspiration graphic novel best of hel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              domain  \\\n",
       "0          https://stackoverflow.com   \n",
       "1           https://en.wikipedia.org   \n",
       "2  https://www.farming-simulator.com   \n",
       "3              https://www.canva.com   \n",
       "4                  https://hello.com   \n",
       "\n",
       "                                           text_dump  HIV_related  \n",
       "0  Stack Overflow Questions Developer Jobs Tags U...            0  \n",
       "1  Cat From Wikipedia, the free encyclopedia \\n\\t...            0  \n",
       "2  Updates Support Australia (en) Belgien (de) Be...            0  \n",
       "3   +1 335  Tweet 714  Share 8.5K  Pin 28.4K  Sha...            0  \n",
       "4  about us inspiration graphic novel best of hel...            0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a label column of 0s indicating the sites are NOT HIV related sites\n",
    "neg_sites1[\"HIV_related\"]=0\n",
    "neg_sites1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the second HIV related sites folder (Positive Class 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON: json_data\n",
    "import json\n",
    "\n",
    "pos_class_list2 = []\n",
    "with open(\"pos_class1.jsonl\") as json_file:\n",
    "   for line in json_file:\n",
    "    pos_class_list2.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>text_dump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.pepfar.gov</td>\n",
       "      <td>Skip to Main Content PEPFAR Seal The United St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.viivhealthcare.com</td>\n",
       "      <td>JavaScript is disabled.Please enable to contin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://hivsa.com</td>\n",
       "      <td>facebook twitter linkedin youtube Home Who We ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.ghfp.net</td>\n",
       "      <td>Jump to navigation Toggle navigation  SIGN IN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.dapp-namibia.org</td>\n",
       "      <td>Register \\r\\n                                 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           domain  \\\n",
       "0          https://www.pepfar.gov   \n",
       "1  https://www.viivhealthcare.com   \n",
       "2                http://hivsa.com   \n",
       "3            https://www.ghfp.net   \n",
       "4     http://www.dapp-namibia.org   \n",
       "\n",
       "                                           text_dump  \n",
       "0  Skip to Main Content PEPFAR Seal The United St...  \n",
       "1  JavaScript is disabled.Please enable to contin...  \n",
       "2  facebook twitter linkedin youtube Home Who We ...  \n",
       "3  Jump to navigation Toggle navigation  SIGN IN ...  \n",
       "4  Register \\r\\n                                 ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sites2=pd.DataFrame(pos_class_list2)\n",
    "pos_sites2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>text_dump</th>\n",
       "      <th>HIV_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.pepfar.gov</td>\n",
       "      <td>Skip to Main Content PEPFAR Seal The United St...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.viivhealthcare.com</td>\n",
       "      <td>JavaScript is disabled.Please enable to contin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://hivsa.com</td>\n",
       "      <td>facebook twitter linkedin youtube Home Who We ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.ghfp.net</td>\n",
       "      <td>Jump to navigation Toggle navigation  SIGN IN ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.dapp-namibia.org</td>\n",
       "      <td>Register \\r\\n                                 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           domain  \\\n",
       "0          https://www.pepfar.gov   \n",
       "1  https://www.viivhealthcare.com   \n",
       "2                http://hivsa.com   \n",
       "3            https://www.ghfp.net   \n",
       "4     http://www.dapp-namibia.org   \n",
       "\n",
       "                                           text_dump  HIV_related  \n",
       "0  Skip to Main Content PEPFAR Seal The United St...            1  \n",
       "1  JavaScript is disabled.Please enable to contin...            1  \n",
       "2  facebook twitter linkedin youtube Home Who We ...            1  \n",
       "3  Jump to navigation Toggle navigation  SIGN IN ...            1  \n",
       "4  Register \\r\\n                                 ...            1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a label column of 1s indicating the sites are HIV related sites\n",
    "pos_sites2[\"HIV_related\"]=1\n",
    "pos_sites2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the second NOT HIV related sites folder (Negative Class2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON: json_data\n",
    "import json\n",
    "\n",
    "neg_class_list2 = []\n",
    "with open(\"neg_class1.jsonl\") as json_file:\n",
    "   for line in json_file:\n",
    "    neg_class_list2.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>text_dump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hello.com</td>\n",
       "      <td>about us inspiration graphic novel best of hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.farming-simulator.com</td>\n",
       "      <td>Updates Support Australia (en) Belgien (de) Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.random.org</td>\n",
       "      <td>Home Games Lottery Quick Pick Keno Quick Pick ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://storify.com</td>\n",
       "      <td>Browse Log In Storify Update We appreciate tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://stackoverflow.com</td>\n",
       "      <td>Stack Overflow Questions Developer Jobs Tags U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              domain  \\\n",
       "0                  https://hello.com   \n",
       "1  https://www.farming-simulator.com   \n",
       "2             https://www.random.org   \n",
       "3                https://storify.com   \n",
       "4          https://stackoverflow.com   \n",
       "\n",
       "                                           text_dump  \n",
       "0  about us inspiration graphic novel best of hel...  \n",
       "1  Updates Support Australia (en) Belgien (de) Be...  \n",
       "2  Home Games Lottery Quick Pick Keno Quick Pick ...  \n",
       "3  Browse Log In Storify Update We appreciate tho...  \n",
       "4  Stack Overflow Questions Developer Jobs Tags U...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_sites2=pd.DataFrame(neg_class_list2)\n",
    "neg_sites2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>text_dump</th>\n",
       "      <th>HIV_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hello.com</td>\n",
       "      <td>about us inspiration graphic novel best of hel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.farming-simulator.com</td>\n",
       "      <td>Updates Support Australia (en) Belgien (de) Be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.random.org</td>\n",
       "      <td>Home Games Lottery Quick Pick Keno Quick Pick ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://storify.com</td>\n",
       "      <td>Browse Log In Storify Update We appreciate tho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://stackoverflow.com</td>\n",
       "      <td>Stack Overflow Questions Developer Jobs Tags U...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              domain  \\\n",
       "0                  https://hello.com   \n",
       "1  https://www.farming-simulator.com   \n",
       "2             https://www.random.org   \n",
       "3                https://storify.com   \n",
       "4          https://stackoverflow.com   \n",
       "\n",
       "                                           text_dump  HIV_related  \n",
       "0  about us inspiration graphic novel best of hel...            0  \n",
       "1  Updates Support Australia (en) Belgien (de) Be...            0  \n",
       "2  Home Games Lottery Quick Pick Keno Quick Pick ...            0  \n",
       "3  Browse Log In Storify Update We appreciate tho...            0  \n",
       "4  Stack Overflow Questions Developer Jobs Tags U...            0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a label column of 0s indicating the sites are NOT HIV related sites\n",
    "neg_sites2[\"HIV_related\"]=0\n",
    "neg_sites2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all the dataframes which were loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 67 entries, 0 to 66\n",
      "Data columns (total 3 columns):\n",
      "domain         67 non-null object\n",
      "text_dump      67 non-null object\n",
      "HIV_related    67 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Merge the negative and the positive class dataframes by append() method\n",
    "sites= pos_sites1.append(neg_sites1).append(pos_sites2).append(neg_sites2)\n",
    "\n",
    "# Reset the index \n",
    "sites.reset_index(drop=True, inplace=True)\n",
    "sites.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>text_dump</th>\n",
       "      <th>HIV_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.hivsharespace.net</td>\n",
       "      <td>Skip to main content En Fr Pt HOME | ABOUT | C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.dapp-namibia.org</td>\n",
       "      <td>Register \\r\\n                                 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.pepfar.gov</td>\n",
       "      <td>Skip to Main Content PEPFAR Seal The United St...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.arasa.info</td>\n",
       "      <td>Home AIDS 2016 About Staff Office Addresses Co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://hivsa.com</td>\n",
       "      <td>facebook twitter linkedin youtube Home Who We ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         domain  \\\n",
       "0  http://www.hivsharespace.net   \n",
       "1   http://www.dapp-namibia.org   \n",
       "2        https://www.pepfar.gov   \n",
       "3         http://www.arasa.info   \n",
       "4              http://hivsa.com   \n",
       "\n",
       "                                           text_dump  HIV_related  \n",
       "0  Skip to main content En Fr Pt HOME | ABOUT | C...            1  \n",
       "1  Register \\r\\n                                 ...            1  \n",
       "2  Skip to Main Content PEPFAR Seal The United St...            1  \n",
       "3  Home AIDS 2016 About Staff Office Addresses Co...            1  \n",
       "4  facebook twitter linkedin youtube Home Who We ...            1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Text Data\n",
    "\n",
    "- Remove **non-alphabetic** characters,\n",
    "- **Remove stop words** \n",
    "- **Lemmatize**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer for CountVectorizer\n",
    "Create a Lemmatizer object to pass in the Sklearn Countvectorizer as `tokenizer` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary nltk modules \n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Create the LemmaTokenizer Object in order to pass in CountVectorizer\n",
    "class LemmaTokenizer(object):\n",
    "    # Initilize WordNetLemmatizer in the class\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        # Tokenize the doc: token_list\n",
    "        tokens_list=word_tokenize(doc)\n",
    "        \n",
    "        # Create a list that iterates through tokens_list and retains only alphabetical characters\n",
    "        # Use the .isalpha() method to check if the character is alphabetical\n",
    "        alpha_token_list= [token for token in tokens_list if token.isalpha()]\n",
    "        \n",
    "        # Return the list after iterating over alpha_tokens_list and lemmatize each token\n",
    "        return [self.lemmatizer.lemmatize(t) for t in alpha_token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "# Specify the argument stop_words=\"english\" so that stop words are removed\n",
    "# Pass in LemmaTokenizer() as an argument\n",
    "count_vectorizer = CountVectorizer(stop_words='english', tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>better</th>\n",
       "      <th>calculaters</th>\n",
       "      <th>calculation</th>\n",
       "      <th>like</th>\n",
       "      <th>wheather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   apple  better  calculaters  calculation  like  wheather\n",
       "0      1       1            1            1     1         1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the outcome of count_vectorizer on a simple data\n",
    "sample= count_vectorizer.fit_transform([\"WheatheR, better apples but i don't like part-2, calculation, calculaters, 21\"])\n",
    "sample_df= pd.DataFrame(sample.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series to store the labels: y\n",
    "y = sites.HIV_related\n",
    "#print(y.head(), \"\\n\")\n",
    "\n",
    "# Create X (features dataset)\n",
    "X= sites.text_dump\n",
    "#print(X.head())\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50x8266 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16383 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform the training data using only X_train ('text_dump' column values): train_dtm\n",
    "# Create train document-term-matrix by using the .fit_transform() method\n",
    "train_dtm = count_vectorizer.fit_transform(X_train)\n",
    "train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to a dense matrix\n",
    "train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aafp</th>\n",
       "      <th>aardwolf</th>\n",
       "      <th>abalone</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abap</th>\n",
       "      <th>abarth</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>abby</th>\n",
       "      <th>abc</th>\n",
       "      <th>aberration</th>\n",
       "      <th>...</th>\n",
       "      <th>吴语</th>\n",
       "      <th>文言</th>\n",
       "      <th>日本語</th>\n",
       "      <th>简体中文</th>\n",
       "      <th>粵語</th>\n",
       "      <th>贛語</th>\n",
       "      <th>ꮳꮃꭹ</th>\n",
       "      <th>한국어</th>\n",
       "      <th>ｍｓ</th>\n",
       "      <th>ｐゴシック</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aafp  aardwolf  abalone  abandonment  abap  abarth  abbreviated  abby  abc  \\\n",
       "0     0         0        0            0     0       0            0     0    0   \n",
       "1     0         0        0            0     0       0            0     0    0   \n",
       "2     0         0        0            0     0       3            0     0    0   \n",
       "3     0         0        0            0     0       0            0     0    0   \n",
       "4     0         0        0            0     0       0            0     0    0   \n",
       "\n",
       "   aberration  ...    吴语  文言  日本語  简体中文  粵語  贛語  ꮳꮃꭹ  한국어  ｍｓ  ｐゴシック  \n",
       "0           0  ...     0   0    0     0   0   0    0    0   0      0  \n",
       "1           0  ...     0   0    0     0   0   0    0    0   0      0  \n",
       "2           0  ...     0   0    0     0   0   0    0    0   0      0  \n",
       "3           0  ...     0   0    0     0   0   0    0    0   0      0  \n",
       "4           0  ...     0   0    0     0   0   0    0    0   0      0  \n",
       "\n",
       "[5 rows x 8266 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the vocabulary and document-term matrix together\n",
    "doc_term_df= pd.DataFrame(train_dtm.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "doc_term_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data (X_test) ie create test document-term-matrix: test_dtm \n",
    "# Use ONLY the .transform() method, NOT fit method\n",
    "test_dtm = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aafp', 'aardwolf', 'abalone', 'abandonment', 'abap', 'abarth', 'abbreviated', 'abby', 'abc', 'aberration', 'abidjan', 'abigail', 'ability', 'able', 'abnormality', 'abril', 'absence', 'absinth', 'absolute', 'absolutely', 'absorption', 'abstract', 'abu', 'abuelo', 'abundance', 'abuse', 'abusing', 'abusive', 'abyssinian', 'abyssinica', 'academia', 'academy', 'accelerate', 'accelerating', 'accelerator', 'accelerometer', 'accent', 'acceptability', 'acceptable', 'access', 'accessed', 'accessibility', 'accessing', 'accident', 'accompanied', 'accomplish', 'accordance', 'according', 'account', 'accountability', 'accountable', 'accredited', 'accrued', 'accueil', 'accuracy', 'accurate', 'accurately', 'accused', 'ace', 'acenta', 'acetyl', 'achap', 'acharya', 'achieve', 'acid', 'acinonyx', 'acknowledgement', 'acne', 'acquire', 'acquiring', 'acre', 'acrobat', 'act', 'acta', 'actinidia', 'action', 'active', 'actively', 'activer', 'activist', 'activity', 'actually', 'acute', 'ad', 'adam', 'adaptable', 'adaptation', 'adapted', 'adapting', 'adaptive', 'add', 'added', 'adding', 'addition', 'additional', 'additive', 'address', 'addressing', 'adequacy', 'adequate', 'adherence', 'adina', 'admin', 'adminhtml', 'administered', 'administration', 'administrative', 'administrator', 'admissible', 'adobe', 'adolescence', 'adolescent', 'adopt', 'adopted', 'adopting', 'adoption', 'adrian', 'adult', 'adustus', 'advance', 'advanced', 'advancing', 'advantage', 'advent', 'adverse', 'advertise', 'advertisement', 'advertising', 'advice', 'advisor', 'advisory', 'advocacy', 'advocate', 'advocating', 'ae', 'aegean', 'aesthetic', 'affair', 'affect', 'affected', 'affecting', 'affection', 'affiliate', 'affiliated', 'afghanistan', 'aficionado', 'africa', 'african', 'africana', 'africats', 'afrikaans', 'aftermath', 'afternoon', 'aftv', 'age', 'aged', 'agency', 'agenda', 'aggregation', 'aggression', 'aggressive', 'aggressiveness', 'aging', 'ago', 'agree', 'agreeing', 'agreement', 'agriaid', 'agriaidsa', 'agricultural', 'agriculture', 'ahead', 'ahmed', 'aho', 'ai', 'aid', 'ailouros', 'ailuridae', 'ailurophobes', 'ailurophobia', 'ailuropoda', 'ailurus', 'aim', 'aimed', 'air', 'airbladegfa', 'airline', 'airport', 'ajwerth', 'akame', 'akihiro', 'akuma', 'al', 'alan', 'aland', 'alaskan', 'albania', 'albicauda', 'albinucha', 'album', 'alcohol', 'alemannisch', 'alexander', 'alexandri', 'alexey', 'alfa', 'algeria', 'algorithm', 'alia', 'alibaba', 'alien', 'aliexpress', 'alimama', 'alios', 'alipay', 'alison', 'alitelecom', 'alive', 'allen', 'alleni', 'alley', 'alliance', 'allocated', 'allow', 'allowed', 'allowing', 'allows', 'allure', 'ally', 'alon', 'alonso', 'alpha', 'alphabetical', 'alphabetically', 'alphanumeric', 'alpinus', 'altaica', 'altered', 'alternate', 'alternation', 'alternative', 'alternet', 'aluminium', 'aluminum', 'alumnus', 'alves', 'amagasaki', 'amazing', 'amazon', 'amb', 'ambassador', 'amber', 'ambitious', 'ambush', 'america', 'american', 'americana', 'americanus', 'ames', 'amethyst', 'amg', 'amicaal', 'amico', 'amino', 'aminotransferase', 'ammonia', 'amped', 'amphibian', 'amplify', 'amusement', 'anakuma', 'analogous', 'analysis', 'anat', 'anatomica', 'anatomical', 'anatomy', 'ancestor', 'ancestral', 'ancestry', 'ancient', 'andean', 'anderson', 'andorra', 'andrea', 'andreessen', 'andrew', 'android', 'andy', 'angeles', 'angie', 'angier', 'angola', 'angolan', 'angolensis', 'angora', 'anguilla', 'angular', 'angustifrons', 'angustirostris', 'animal', 'animalia', 'animalis', 'animating', 'animation', 'anime', 'aninsight', 'anise', 'aniseed', 'annan', 'anne', 'annetta', 'annie', 'anniversary', 'announce', 'announcement', 'announces', 'annual', 'annually', 'annunciation', 'anorexia', 'ansorgei', 'answer', 'answered', 'antarctic', 'antarctica', 'anthony', 'anthropologist', 'anthropology', 'anthrozoos', 'anthus', 'antiche', 'antigua', 'antique', 'antiskidding', 'antunes', 'anzala', 'ao', 'aonyx', 'apac', 'apache', 'apf', 'api', 'app', 'appalls', 'apparel', 'appeal', 'appear', 'appearance', 'appears', 'applaud', 'apple', 'appleton', 'appliance', 'application', 'applied', 'apply', 'appointed', 'appointment', 'appreciate', 'appreciated', 'approach', 'approaching', 'approval', 'apps', 'apr', 'april', 'apéritifs', 'aqua', 'aquatic', 'ar', 'arab', 'arabia', 'arabian', 'arabic', 'arachidonate', 'arachidonic', 'aragonés', 'arasa', 'archaeologist', 'arched', 'arching', 'archive', 'archived', 'arctic', 'arctictis', 'arctocephalus', 'arctogalidia', 'arctonyx', 'arctos', 'arduino', 'area', 'aren', 'argentina', 'arginine', 'argwings', 'arial', 'ariana', 'arisen', 'arising', 'aristotle', 'arizona', 'arlington', 'arm', 'armenia', 'armes', 'armãneashti', 'arnold', 'aroma', 'aromatic', 'aromatics', 'aronson', 'arpetan', 'arqade', 'arranged', 'array', 'arrived', 'arsenal', 'arsene', 'arsenic', 'art', 'artemis', 'arthropod', 'arthur', 'article', 'artificial', 'artist', 'artisteer', 'artistree', 'arturo', 'aru', 'aruba', 'arvs', 'ascension', 'ascent', 'ascribing', 'asean', 'ash', 'asia', 'asian', 'asiatic', 'aside', 'ask', 'asked', 'aslan', 'asleep', 'asocial', 'asos', 'aspca', 'aspect', 'ass', 'assassination', 'assemble', 'assembled', 'assembly', 'assessment', 'asset', 'assign', 'assist', 'assistance', 'assistant', 'associate', 'associated', 'association', 'associazione', 'assumes', 'assurance', 'assured', 'asthma', 'aston', 'astonishment', 'asturianu', 'astutus', 'asychronous', 'ataxia', 'ateca', 'atelocynus', 'atikamekw', 'atilax', 'atlantic', 'atlas', 'atmospheric', 'atom', 'attached', 'attachment', 'attack', 'attacking', 'attempt', 'attempted', 'attempting', 'attend', 'attended', 'attending', 'attention', 'attested', 'attitude', 'attract', 'attracted', 'attraction', 'attractive', 'attracts', 'attributable', 'attributed', 'attribution', 'atypical', 'audi', 'audience', 'audio', 'auditory', 'aug', 'augment', 'august', 'aurata', 'aureus', 'aussie', 'austin', 'australasia', 'australasian', 'australia', 'australian', 'australis', 'austria', 'authorised', 'authority', 'autistic', 'auto', 'automated', 'automatic', 'automatically', 'automation', 'automobile', 'automotive', 'autonavi', 'autophagy', 'autumn', 'availability', 'available', 'avec', 'avenger', 'average', 'averaging', 'avert', 'avertblog', 'avifauna', 'avifaunal', 'aviv', 'avma', 'avoid', 'award', 'awarded', 'aware', 'awareness', 'away', 'awesome', 'awful', 'ayashiya', 'ayisyen', 'aymar', 'azerbaijan', 'azərbaycanca', 'años', 'b', 'baby', 'backdrop', 'backed', 'background', 'bacon', 'bacteria', 'bad', 'badger', 'badia', 'bag', 'bahamas', 'bahasa', 'bahrain', 'baikal', 'balance', 'balanced', 'balinese', 'ballislife', 'balázs', 'bamanankan', 'ban', 'band', 'banded', 'bangladesh', 'bank', 'banned', 'banner', 'banquet', 'bantam', 'baragwanath', 'barbados', 'barbara', 'barbarian', 'barbatus', 'barbeque', 'barbuda', 'bare', 'barisan', 'barley', 'baron', 'barratt', 'barrel', 'barrier', 'barron', 'bartender', 'bartending', 'barthelemy', 'bartleby', 'barware', 'basa', 'base', 'baseball', 'based', 'baseline', 'bash', 'basic', 'basis', 'basket', 'bass', 'bassaricyon', 'bassariscus', 'bastet', 'batch', 'bateson', 'battered', 'battle', 'bauer', 'bay', 'baysac', 'bbbee', 'bbc', 'bc', 'bdeogale', 'bea', 'beach', 'beachy', 'beadle', 'bean', 'bear', 'bearded', 'beat', 'beating', 'beautiful', 'beauty', 'becker', 'beckerman', 'beckett', 'beckham', 'beech', 'beer', 'beginning', 'behalf', 'behav', 'behave', 'behavior', 'behavioral', 'behaviorally', 'behaviour', 'behavioural', 'behrend', 'bekoff', 'belarus', 'belgien', 'belgique', 'belgium', 'belgië', 'belief', 'believe', 'believed', 'believing', 'belize', 'bell', 'belly', 'belong', 'beloved', 'benchmark', 'bender', 'benecke', 'beneficiary', 'benefit', 'bengal', 'bengalensis', 'benhamou', 'benin', 'bennettii', 'bentley', 'berber', 'bergstrom', 'berlin', 'berlingo', 'bermuda', 'berry', 'best', 'bet', 'bethlehem', 'better', 'bettong', 'beyonce', 'bharath', 'bhutan', 'biagi', 'biannual', 'bibcode', 'bibliography', 'bicolor', 'bictegravir', 'bicubic', 'bicycle', 'bieti', 'big', 'bigdata', 'bigger', 'biggest', 'bikol', 'billing', 'billion', 'bin', 'binding', 'binomial', 'binotata', 'binturong', 'biochem', 'biochemical', 'biodiversity', 'biogen', 'biography', 'biol', 'biological', 'biology', 'biomechanics', 'biometric', 'bionda', 'biosynthesize', 'biourge', 'bipartisan', 'birch', 'bird', 'birdie', 'birdlife', 'birman', 'birth', 'birthday', 'bisexual', 'bit', 'bitcoin', 'bite', 'biting', 'bitmap', 'bitten', 'bitter', 'biturbo', 'bizaad', 'black', 'blacklivesmatter', 'blade', 'blanc', 'blanford', 'blanket', 'blastocyst', 'bleach', 'blend', 'blender', 'blenkinsop', 'blindness', 'blink', 'blinkmacsystemfont', 'bloat', 'block', 'blocked', 'blog', 'blogger', 'blogging', 'blood', 'blossom', 'blowout', 'blue', 'bluehdi', 'bluish', 'blumberg', 'bmt', 'bmw', 'board', 'boarisch', 'boat', 'boater', 'boating', 'bobcat', 'bobtail', 'bocchan', 'bock', 'body', 'boeing', 'boivin', 'bold', 'bolder', 'bolivarian', 'bolivia', 'bombay', 'bonaire', 'bonding', 'bone', 'bonfire', 'book', 'bookmark', 'bool', 'boost', 'boosterjet', 'boot', 'bootcamps', 'booting', 'border', 'boring', 'born', 'bornean', 'boroneant', 'borrowed', 'borrowing', 'bos', 'bosanski', 'boshel', 'bosnia', 'boston', 'bot', 'botany', 'botswana', 'bottle', 'bough', 'bought', 'bourbon', 'bourlon', 'bourloni', 'bouvet', 'box', 'boy', 'bozebeats', 'bp', 'brachyurus', 'bradler', 'bradshaw', 'brain', 'branca', 'branch', 'brand', 'branding', 'brasileiro', 'brasiliensis', 'brazil', 'brazilian', 'break', 'breaker', 'breakfast', 'breaking', 'breakout', 'breakpoint', 'breakpoints', 'breath', 'breathing', 'bred', 'breed', 'breeding', 'brentford', 'brewery', 'brezhoneg', 'brian', 'brick', 'bride', 'bridging', 'brief', 'bright', 'brightens', 'brighter', 'brights', 'brill', 'brilliant', 'bring', 'brings', 'brink', 'britain', 'british', 'broad', 'broadcast', 'broke', 'broker', 'brokerage', 'brook', 'brophy', 'brought', 'brown', 'browsable', 'browse', 'browser', 'bruce', 'brunei', 'brunnea', 'bruno', 'brush', 'brutal', 'bryan', 'bryant', 'brüel', 'bubastis', 'bubblegum', 'bucket', 'bud', 'budget', 'buffalo', 'buffer', 'buffington', 'build', 'builder', 'building', 'built', 'buitenhuis', 'bukusi', 'bulb', 'bulgaria', 'bulk', 'bull', 'bulletin', 'bullitt', 'bummer', 'bunch', 'burden', 'bureau', 'burial', 'buried', 'burkina', 'burlington', 'burmese', 'burn', 'burned', 'burning', 'burnout', 'burnt', 'burrow', 'burst', 'burundi', 'busch', 'bush', 'bushman', 'business', 'businesslike', 'busted', 'busy', 'butler', 'butterfly', 'button', 'buy', 'buying', 'buzzfeed', 'bvrla', 'byers', 'bygate', 'byte', 'byzantine', 'bălăşescu', 'c', 'ca', 'cabi', 'cable', 'cacao', 'cacomistle', 'cadillac', 'cadorniga', 'cafazzo', 'cafe', 'café', 'cafés', 'cage', 'caicos', 'cake', 'calabash', 'calc', 'calcium', 'calculated', 'calculation', 'calculator', 'caledonia', 'calendar', 'calender', 'caliber', 'calicivirus', 'calico', 'california', 'californian', 'californianus', 'called', 'calling', 'callorhinus', 'calming', 'cambodia', 'cambridge', 'came', 'camel', 'camera', 'cameroon', 'cami', 'camp']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 1000 features of the count_vectorizer\n",
    "# using .get_feature_names() method\n",
    "print(count_vectorizer.get_feature_names()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import MultinomialNB from sklearn.naive_bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Import the metrics module from sklearn \n",
    "from sklearn import metrics\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training document term matrix\n",
    "nb_classifier.fit(train_dtm, y_train)\n",
    "\n",
    "# Compute the predicted tags for the test_data: pred\n",
    "pred = nb_classifier.predict(test_dtm)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score for the classifier: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7, 0.8, 0.8, 0.8, 0.8])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute 5-fold cross-validation scores: cv_scores\n",
    "cv_scores = cross_val_score(nb_classifier, train_dtm, y_train, cv=5)\n",
    "\n",
    "# Print the 5-fold cross-validation scores\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  0],\n",
       "       [ 1, 11]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the confusion matrix\n",
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: text_dump, dtype: object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the site contents for the false positives (Irrelevant sites incorrectly classified as HIV related)\n",
    "X_test[y_test < pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37    For full functionality of this site it is nece...\n",
       "Name: text_dump, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the site contents for the false negatives (HIV related sites incorrectly classified as Irrelevant)\n",
    "X_test[y_test > pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.78361356e-043, 3.02807761e-255, 1.00000000e+000, 1.03268215e-111,\n",
       "       1.00000000e+000, 1.00000000e+000, 1.00000000e+000, 1.00000000e+000,\n",
       "       1.00000000e+000, 1.00000000e+000, 8.38096543e-083, 4.02415896e-049,\n",
       "       1.00000000e+000, 0.00000000e+000, 1.00000000e+000, 1.00000000e+000,\n",
       "       1.00000000e+000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate predicted probabilities for X_test (poorly calibrated)\n",
    "y_pred_prob = nb_classifier.predict_proba(test_dtm)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.79484388 -9.79484388 -9.79484388 ... -9.79484388 -9.79484388\n",
      "  -9.79484388]]\n"
     ]
    }
   ],
   "source": [
    "print(nb_classifier.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a Pipline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        s...78>,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the token pattern in order to include only alpha characters: TOKENS_ALPHA_ONLY \n",
    "TOKENS_ALPHA_ONLY = '[A-Za-z]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate Pipeline object: pl\n",
    "# Add the 'CountVectorizer' step (with the name 'vec') to the correct position in the pipeline\n",
    "pl = Pipeline([\n",
    "               ('vec', CountVectorizer(stop_words='english', tokenizer=LemmaTokenizer())),\n",
    "               ('clf', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "# Fit the Pipeline to the training data\n",
    "pl.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
